{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './data/train/'\n",
    "LOGS_Path = \"./logs\"\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = glob.glob(os.path.join(TRAIN_PATH,\"**/*\"))\n",
    "def get_img_batch(files_list,batch_size=32,size=(224,224),should_normalise=True,should_noise=True):\n",
    "   \n",
    "    batch_cover = []\n",
    "    batch_secret = []\n",
    "\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img_secret_path = random.choice(files_list)\n",
    "        img_cover_path = random.choice(files_list)\n",
    "        \n",
    "        img_secret = np.array(Image.open(img_secret_path).resize(size).convert(\"RGB\"),dtype=np.float32)\n",
    "        img_cover = np.array(Image.open(img_cover_path).resize(size).convert(\"RGB\"),dtype=np.float32)\n",
    "        \n",
    "        img_secret /= 255.\n",
    "        img_cover /= 255.\n",
    "        \n",
    "        batch_cover.append(img_cover)\n",
    "        batch_secret.append(img_secret)\n",
    "        \n",
    "    return np.array(batch_cover) , np.array(batch_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_network(secret_tensor):\n",
    "    \n",
    "    with tf.variable_scope('prep_net'):\n",
    "        \n",
    "        with tf.variable_scope(\"3x3_conv_branch\"):\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=secret_tensor,filters=50,kernel_size=3,padding='same',name=\"1\")\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=conv_3x3,filters=50,kernel_size=3,padding='same',name=\"2\")\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=conv_3x3,filters=50,kernel_size=3,padding='same',name=\"3\")\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=conv_3x3,filters=50,kernel_size=3,padding='same',name=\"4\")\n",
    "            \n",
    "        with tf.variable_scope(\"4x4_conv_branch\"):\n",
    "            conv_4x4 = tf.layers.conv2d(inputs=secret_tensor,filters=50,kernel_size=4,padding='same',name=\"1\")\n",
    "            conv_4x4 = tf.layers.conv2d(inputs=conv_4x4,filters=50,kernel_size=4,padding='same',name=\"2\")            \n",
    "            conv_4x4 = tf.layers.conv2d(inputs=conv_4x4,filters=50,kernel_size=4,padding='same',name=\"3\")\n",
    "            conv_4x4 = tf.layers.conv2d(inputs=conv_4x4,filters=50,kernel_size=4,padding='same',name=\"4\")\n",
    "\n",
    "        with tf.variable_scope(\"5x5_conv_branch\"):\n",
    "            conv_5x5 = tf.layers.conv2d(inputs=secret_tensor,filters=50,kernel_size=5,padding='same',name=\"1\")\n",
    "            conv_5x5 = tf.layers.conv2d(inputs=conv_5x5,filters=50,kernel_size=5,padding='same',name=\"2\")            \n",
    "            conv_5x5 = tf.layers.conv2d(inputs=conv_5x5,filters=50,kernel_size=5,padding='same',name=\"3\")\n",
    "            conv_5x5 = tf.layers.conv2d(inputs=conv_5x5,filters=50,kernel_size=5,padding='same',name=\"4\")\n",
    "            \n",
    "        concat_1 = tf.concat([conv_3x3,conv_4x4,conv_5x5],axis=3,name='concat_1')\n",
    "        \n",
    "        conv_5x5 = tf.layers.conv2d(inputs=concat_1,filters=50,kernel_size=5,padding='same',name=\"final_5x5\")\n",
    "        conv_4x4 = tf.layers.conv2d(inputs=concat_1,filters=50,kernel_size=5,padding='same',name=\"final_4x4\")\n",
    "        conv_3x3 = tf.layers.conv2d(inputs=concat_1,filters=50,kernel_size=5,padding='same',name=\"final_3x3\")\n",
    "        \n",
    "        concat_final = tf.concat([conv_5x5,conv_4x4,conv_3x3],axis=3,name='concat_final')\n",
    "\n",
    "\n",
    "        return concat_final\n",
    "\n",
    "    \n",
    "def hiding_network(cover_tensor,prep_output):\n",
    "    \n",
    "    with tf.variable_scope('hide_net'):\n",
    "        concat_input = tf.concat([cover_tensor,prep_output],axis=3,name='images_features_concat')\n",
    "        \n",
    "        with tf.variable_scope(\"3x3_conv_branch\"):\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=concat_input,filters=50,kernel_size=3,padding='same',name=\"1\")\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=conv_3x3,filters=50,kernel_size=3,padding='same',name=\"2\")\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=conv_3x3,filters=50,kernel_size=3,padding='same',name=\"3\")\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=conv_3x3,filters=50,kernel_size=3,padding='same',name=\"4\")\n",
    "            \n",
    "        with tf.variable_scope(\"4x4_conv_branch\"):\n",
    "            conv_4x4 = tf.layers.conv2d(inputs=concat_input,filters=50,kernel_size=4,padding='same',name=\"1\")\n",
    "            conv_4x4 = tf.layers.conv2d(inputs=conv_4x4,filters=50,kernel_size=4,padding='same',name=\"2\")            \n",
    "            conv_4x4 = tf.layers.conv2d(inputs=conv_4x4,filters=50,kernel_size=4,padding='same',name=\"3\")\n",
    "            conv_4x4 = tf.layers.conv2d(inputs=conv_4x4,filters=50,kernel_size=4,padding='same',name=\"4\")\n",
    "\n",
    "        with tf.variable_scope(\"5x5_conv_branch\"):\n",
    "            conv_5x5 = tf.layers.conv2d(inputs=concat_input,filters=50,kernel_size=5,padding='same',name=\"1\")\n",
    "            conv_5x5 = tf.layers.conv2d(inputs=conv_5x5,filters=50,kernel_size=5,padding='same',name=\"2\")            \n",
    "            conv_5x5 = tf.layers.conv2d(inputs=conv_5x5,filters=50,kernel_size=5,padding='same',name=\"3\")\n",
    "            conv_5x5 = tf.layers.conv2d(inputs=conv_5x5,filters=50,kernel_size=5,padding='same',name=\"4\")\n",
    "            \n",
    "        concat_1 = tf.concat([conv_3x3,conv_4x4,conv_5x5],axis=3,name='concat_1')\n",
    "        \n",
    "        conv_5x5 = tf.layers.conv2d(inputs=concat_1,filters=50,kernel_size=5,padding='same',name=\"final_5x5\")\n",
    "        conv_4x4 = tf.layers.conv2d(inputs=concat_1,filters=50,kernel_size=5,padding='same',name=\"final_4x4\")\n",
    "        conv_3x3 = tf.layers.conv2d(inputs=concat_1,filters=50,kernel_size=5,padding='same',name=\"final_3x3\")\n",
    "        \n",
    "        concat_final = tf.concat([conv_5x5,conv_4x4,conv_3x3],axis=3,name='concat_final')\n",
    "    \n",
    "        output = tf.layers.conv2d(inputs=concat_final,filters=3,kernel_size=3,padding='same',name='output')\n",
    "\n",
    "\n",
    "\n",
    "        return output\n",
    "    \n",
    "        \n",
    "        \n",
    "def reveal_network(container_tensor):\n",
    "    \n",
    "    with tf.variable_scope('reveal_net'):\n",
    "        \n",
    "        with tf.variable_scope(\"3x3_conv_branch\"):\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=container_tensor,filters=50,kernel_size=3,padding='same',name=\"1\")\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=conv_3x3,filters=50,kernel_size=3,padding='same',name=\"2\")\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=conv_3x3,filters=50,kernel_size=3,padding='same',name=\"3\")\n",
    "            conv_3x3 = tf.layers.conv2d(inputs=conv_3x3,filters=50,kernel_size=3,padding='same',name=\"4\")\n",
    "            \n",
    "        with tf.variable_scope(\"4x4_conv_branch\"):\n",
    "            conv_4x4 = tf.layers.conv2d(inputs=container_tensor,filters=50,kernel_size=4,padding='same',name=\"1\")\n",
    "            conv_4x4 = tf.layers.conv2d(inputs=conv_4x4,filters=50,kernel_size=4,padding='same',name=\"2\")            \n",
    "            conv_4x4 = tf.layers.conv2d(inputs=conv_4x4,filters=50,kernel_size=4,padding='same',name=\"3\")\n",
    "            conv_4x4 = tf.layers.conv2d(inputs=conv_4x4,filters=50,kernel_size=4,padding='same',name=\"4\")\n",
    "\n",
    "        with tf.variable_scope(\"5x5_conv_branch\"):\n",
    "            conv_5x5 = tf.layers.conv2d(inputs=container_tensor,filters=50,kernel_size=5,padding='same',name=\"1\")\n",
    "            conv_5x5 = tf.layers.conv2d(inputs=conv_5x5,filters=50,kernel_size=5,padding='same',name=\"2\")            \n",
    "            conv_5x5 = tf.layers.conv2d(inputs=conv_5x5,filters=50,kernel_size=5,padding='same',name=\"3\")\n",
    "            conv_5x5 = tf.layers.conv2d(inputs=conv_5x5,filters=50,kernel_size=5,padding='same',name=\"4\")\n",
    "            \n",
    "        concat_1 = tf.concat([conv_3x3,conv_4x4,conv_5x5],axis=3,name='concat_1')\n",
    "        \n",
    "        conv_5x5 = tf.layers.conv2d(inputs=concat_1,filters=50,kernel_size=5,padding='same',name=\"final_5x5\")\n",
    "        conv_4x4 = tf.layers.conv2d(inputs=concat_1,filters=50,kernel_size=5,padding='same',name=\"final_4x4\")\n",
    "        conv_3x3 = tf.layers.conv2d(inputs=concat_1,filters=50,kernel_size=5,padding='same',name=\"final_3x3\")\n",
    "        \n",
    "        concat_final = tf.concat([conv_5x5,conv_4x4,conv_3x3],axis=3,name='concat_final')\n",
    "    \n",
    "    output = tf.layers.conv2d(inputs=concat_final,filters=3,kernel_size=3,padding='same',name='output')\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_loss_op(secret_true,secret_pred,cover_true,cover_pred,beta=.25):\n",
    "    \n",
    "    secret_mse = tf.losses.mean_squared_error(secret_true,secret_pred)\n",
    "    cover_mse = tf.losses.mean_squared_error(cover_true,cover_pred)\n",
    "    final_loss = cover_mse + beta*secret_mse\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_img_batch(files_list=files_list,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sess = tf.InteractiveSession(graph=tf.Graph())\n",
    "\n",
    "\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "secret_tensor = tf.placeholder(shape=[None,224,224,3],dtype=tf.float32,name=\"input_prep\")\n",
    "cover_tensor = tf.placeholder(shape=[None,224,224,3],dtype=tf.float32,name=\"input_hide\")\n",
    "\n",
    "\n",
    "prep_output_op = prep_network(secret_tensor)\n",
    "hiding_output_op = hiding_network(cover_tensor=cover_tensor,prep_output=prep_output_op)\n",
    "reveal_output_op = reveal_network(hiding_output_op)\n",
    "\n",
    "\n",
    "loss_op = get_loss_op(secret_tensor,reveal_output_op,cover_tensor,hiding_output_op)\n",
    "\n",
    "\n",
    "\n",
    "tf.summary.scalar('cost_value', loss_op)\n",
    "tf.summary.image('input_secret',secret_tensor)\n",
    "tf.summary.image('input_cover',cover_tensor)\n",
    "tf.summary.image('output_cover',hiding_output_op)\n",
    "tf.summary.image('reveal_output',reveal_output_op)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "\n",
    "writer = tf.summary.FileWriter(f\"logs/{datetime.now()}\",sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minimize_op = tf.train.AdamOptimizer(0.0002).minimize(loss_op)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10128\n",
      "WARNING:tensorflow:Ignoring: ./checkpoints/Epoch-0--Loss0.08404688537120819\n",
      "0.087528\n",
      "WARNING:tensorflow:Ignoring: ./checkpoints/Epoch-0--Loss0.08534139394760132\n",
      "0.07919\n",
      "WARNING:tensorflow:Ignoring: ./checkpoints/Epoch-0--Loss0.08790577948093414\n",
      "0.0731602\n",
      "WARNING:tensorflow:Ignoring: ./checkpoints/Epoch-0--Loss0.13042841851711273\n",
      "0.0715436\n",
      "WARNING:tensorflow:Ignoring: ./checkpoints/Epoch-0--Loss0.09077993780374527\n",
      "0.0774648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a098baf4a64c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#     if ep % 300 ==0 :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"./checkpoints/Epoch-{ep}--Loss{loss}.chpk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1494\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m         clear_extraneous_savers=clear_extraneous_savers)\n\u001b[0m\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, **kwargs)\u001b[0m\n\u001b[1;32m   1765\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1768\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mcreate_meta_graph_def\u001b[0;34m(meta_info_def, graph_def, saver_def, collection_list, graph, export_scope, exclude_nodes, clear_extraneous_savers)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m   \u001b[0;31m# Fills in meta_info_def.stripped_op_list using the ops from graph_def.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ep in range(3000):\n",
    "    \n",
    "    \n",
    "    for step in range(len(files_list)//BATCH_SIZE + 1):\n",
    "        covers,secrets = get_img_batch(files_list=files_list,batch_size=BATCH_SIZE)\n",
    "\n",
    "        _,loss = sess.run([minimize_op,loss_op],\n",
    "                                  feed_dict={secret_tensor:secrets,cover_tensor:covers})\n",
    "    \n",
    "        if ep % 10 ==0 :\n",
    "            print(loss)\n",
    "            summary = sess.run(merged_summary_op,feed_dict={secret_tensor:secrets,cover_tensor:covers})\n",
    "            writer.add_summary(summary,ep)\n",
    "        \n",
    "#     if ep % 300 ==0 : \n",
    "        save_path = saver.save(sess, f\"./checkpoints/Epoch-{ep}--Loss{loss}.chpk\")\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()\n",
    "\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers,secrets = batch\n",
    "cover = covers.squeeze()\n",
    "secret = secrets.squeeze()\n",
    "plt.imshow(cover)\n",
    "plt.show()\n",
    "plt.imshow(secret)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_placeholder = tf.placeholder(shape=[1,224,224,3],dtype=tf.float32)\n",
    "with tf.variable_scope(\"\",reuse=True) :\n",
    "    reveal_output_test = reveal_network(test_placeholder)\n",
    "\n",
    "batch2 = get_img_batch(files_list=files_list,batch_size=1) \n",
    "covers,secrets = batch2\n",
    "cover = covers.squeeze()\n",
    "secret = secrets.squeeze()\n",
    "\n",
    "plt.imshow(cover)\n",
    "plt.show()\n",
    "plt.imshow(secret)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "reveal_img,hiding_output = sess.run([reveal_output_op,hiding_output_op],feed_dict={cover_tensor:covers,secret_tensor:secrets})\n",
    "# reveal_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.clip(reveal_img.squeeze(),0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.clip(hiding_output.squeeze(),0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
